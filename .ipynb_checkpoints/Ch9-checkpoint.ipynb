{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch9:Up and Running with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Creating Your First Granph and Running It in a Session](#Creating-Your-First-Granph-and-Running-It-in-a-Session)\n",
    "* [Lifecycle of Node Value](#Lifecycle-of-Node-Value)\n",
    "* [Linear Regression with TensorFlow](#Linear-Regression-with-TensorFlow)\n",
    "* [Implementing Gradient Descent](#Implementing Gradient Descent)\n",
    "\t* [Manually Computing the Gradients](#Manually-Computing-the-Gradients)\n",
    "\t* [Using autodiff](#Using-autodiff)\n",
    "\t* [Using an Optimizer](#Using-an-Optimizer)\n",
    "* [Feeding Data to the Training Algorithm](#Feeding-Data-to-the-Training-Algorithm)\n",
    "* [Saving and Restoring Models](#Saving-and-Restoring-Models)\n",
    "* [Visualizing the Graph and Training Curves Using TensorBoard](#Visualizing-the-Graph-and-Training-Curves-Using-TensorBoard)\n",
    "* [Name Scopes](#Name-Scopes)\n",
    "* [Modularity](#Modularity)\n",
    "* [Sharing Variables](#Sharing-Variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your First Granph and Running It in a Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt](./9_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x=tf.Variable(3,name=\"X\")\n",
    "\n",
    "y=tf.Variable(4,name=\"y\")\n",
    "f=x*x*y+y+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result=sess.run(f)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result=f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result=f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifecyle of a Node Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w=tf.constant(3)\n",
    "x=w+2\n",
    "y=x+5\n",
    "z=x*3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val,z_val=sess.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.74651413e+01]\n",
      " [  4.35734153e-01]\n",
      " [  9.33829229e-03]\n",
      " [ -1.06622010e-01]\n",
      " [  6.44106984e-01]\n",
      " [ -4.25131839e-06]\n",
      " [ -3.77322501e-03]\n",
      " [ -4.26648885e-01]\n",
      " [ -4.40514028e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing=fetch_california_housing()\n",
    "m,n=housing.data.shape\n",
    "housing_data_plus_bias=np.c_[np.ones((m,1)),housing.data]\n",
    "X=tf.constant(housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "\n",
    "XT=tf.transpose(X)\n",
    "theta=tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value=theta.eval()\n",
    "print(theta_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Computing the Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler=StandardScaler()\n",
    "scaled_housing_data=std_scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias=np.c_[np.ones((m,1)),scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 8.2536\n",
      "Epoch 100 MSE= 0.74209\n",
      "Epoch 200 MSE= 0.601004\n",
      "Epoch 300 MSE= 0.583378\n",
      "Epoch 400 MSE= 0.572428\n",
      "Epoch 500 MSE= 0.563737\n",
      "Epoch 600 MSE= 0.556689\n",
      "Epoch 700 MSE= 0.550949\n",
      "Epoch 800 MSE= 0.546262\n",
      "Epoch 900 MSE= 0.542426\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "gradients=2/m*tf.matmul(tf.transpose(X),error)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100==0:\n",
    "            print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 10.6348\n",
      "Epoch 100 MSE= 0.695306\n",
      "Epoch 200 MSE= 0.530887\n",
      "Epoch 300 MSE= 0.527008\n",
      "Epoch 400 MSE= 0.52622\n",
      "Epoch 500 MSE= 0.525692\n",
      "Epoch 600 MSE= 0.525311\n",
      "Epoch 700 MSE= 0.525037\n",
      "Epoch 800 MSE= 0.524838\n",
      "Epoch 900 MSE= 0.524695\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "gradients=tf.gradients(mse,[theta])[0]\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100==0:\n",
    "            print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE= 6.15226\n",
      "Epoch 100 MSE= 0.801498\n",
      "Epoch 200 MSE= 0.687328\n",
      "Epoch 300 MSE= 0.652995\n",
      "Epoch 400 MSE= 0.627381\n",
      "Epoch 500 MSE= 0.607236\n",
      "Epoch 600 MSE= 0.591277\n",
      "Epoch 700 MSE= 0.578573\n",
      "Epoch 800 MSE= 0.568417\n",
      "Epoch 900 MSE= 0.560264\n"
     ]
    }
   ],
   "source": [
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100==0:\n",
    "            print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=tf.placeholder(tf.float32,shape=(None,3))\n",
    "B=A+5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1=B.eval(feed_dict={A:[[1,2,3]]})\n",
    "    B_val_2=B.eval(feed_dict={A:[[4,5,6],[7,8,9]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(batch_size):\n",
    "    shuffle_indices=np.random.permutation(len(scaled_housing_data_plus_bias))\n",
    "    X_batch_indices=shuffle_indices[:batch_size]\n",
    "    y_batch_indices=shuffle_indices[:batch_size]\n",
    "    return scaled_housing_data_plus_bias[X_batch_indices],housing.target[y_batch_indices].reshape(-1,1)\n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "batch_size=100\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=(None,n+1),name=\"X\")\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name=\"y\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=fetch_batch(batch_size)\n",
    "            if epoch %100==0:\n",
    "                #print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "                pass\n",
    "            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.06855249]\n",
      " [ 0.7787146 ]\n",
      " [ 0.14920233]\n",
      " [-0.09454302]\n",
      " [ 0.13140813]\n",
      " [ 0.00723424]\n",
      " [-0.04081651]\n",
      " [-0.72569853]\n",
      " [-0.68681014]]\n"
     ]
    }
   ],
   "source": [
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models\n",
    "Nothing to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20180126022357/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "print(logdir) \n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "\n",
    "mse_summary=tf.summary.scalar('MSE',mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100==0:\n",
    "            #print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "            summary_str=mse_summary.eval()\n",
    "            file_writer.add_summary(summary_str,epoch)\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20180126021650/\n",
      "Epoch 0 MSE= 15.5801\n",
      "Epoch 100 MSE= 0.774741\n",
      "Epoch 200 MSE= 0.560504\n",
      "Epoch 300 MSE= 0.550073\n",
      "Epoch 400 MSE= 0.545522\n",
      "Epoch 500 MSE= 0.541912\n",
      "Epoch 600 MSE= 0.538932\n",
      "Epoch 700 MSE= 0.536462\n",
      "Epoch 800 MSE= 0.534413\n",
      "Epoch 900 MSE= 0.532713\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "print(logdir)\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "theta=tf.Variable(np.random.randn(9,1),dtype=tf.float32,name=\"theta\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "   \n",
    "error=y_pred-y\n",
    "mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "\n",
    "mse_summary=tf.summary.scalar('MSE',mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "gradients=2/m*tf.matmul(tf.transpose(X),error)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100==0:\n",
    "            print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "            summary_str=mse_summary.eval()\n",
    "            file_writer.add_summary(summary_str,epoch)\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.06855249]\n",
      " [ 1.02475023]\n",
      " [ 0.18495777]\n",
      " [-0.57879466]\n",
      " [ 0.54037702]\n",
      " [ 0.01704878]\n",
      " [-0.04908734]\n",
      " [-0.23402473]\n",
      " [-0.22471087]]\n"
     ]
    }
   ],
   "source": [
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20180126022753/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "print(logdir) \n",
    "\n",
    "n_epochs=1000\n",
    "learning_rate=0.01\n",
    "X=tf.constant(scaled_housing_data_plus_bias,dtype=tf.float32,name=\"X\")\n",
    "y=tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name=\"y\")\n",
    "y_pred=tf.matmul(X,theta,name=\"predictions\")\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error=y_pred-y\n",
    "    mse=tf.reduce_mean(tf.square(error),name=\"mse\")\n",
    "\n",
    "\n",
    "mse_summary=tf.summary.scalar('MSE',mse)\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n",
    "\n",
    "optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "training_op=optimizer.minimize(mse)\n",
    "training_op=tf.assign(theta,theta-learning_rate*gradients)\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch %100==0:\n",
    "            #print(\"Epoch\",epoch,\"MSE=\",mse.eval())\n",
    "            summary_str=mse_summary.eval()\n",
    "            file_writer.add_summary(summary_str,epoch)\n",
    "        sess.run(training_op)\n",
    "    best_theta=theta.eval()\n",
    "\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20180126030220/\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    w_shape=(int(X.get_shape()[1]),1)\n",
    "    w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "    b=tf.Variable(0.0,name=\"bias\")\n",
    "    z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "    return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "print(logdir)\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20180126030457/\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape=(int(X.get_shape()[1]),1)\n",
    "        w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "        b=tf.Variable(0.0,name=\"bias\")\n",
    "        z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "        return tf.maximum(z,0.,name=\"relu\")\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "print(logdir)\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_logs/run-20180126033040/\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        with tf.variable_scope(\"relu\",reuse=True):\n",
    "            threshold=tf.get_variable(\"threshold\")\n",
    "            w_shape=(int(X.get_shape()[1]),1)\n",
    "            w=tf.Variable(tf.random_normal(w_shape),name=\"weights\")\n",
    "            b=tf.Variable(0.0,name=\"bias\")\n",
    "            z=tf.add(tf.matmul(X,w),b,name=\"z\")\n",
    "            return tf.maximum(z,0.,name=\"max\")\n",
    "\n",
    "now=datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir=\"tf_logs\"\n",
    "logdir=\"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "print(logdir)\n",
    "\n",
    "n_features=3\n",
    "X=tf.placeholder(tf.float32,shape=(None,n_features),name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold=tf.get_variable(\"threshold\",shape=(),initializer=tf.constant_initializer(0.0))\n",
    "relus=[relu(X) for i in range(5)]\n",
    "output=tf.add_n(relus,name=\"output\")\n",
    "\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
